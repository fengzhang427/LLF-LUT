# General settings
name: train-MIT-FiveK-4K
use_tb_logger: true #true
model: LLF_LUT
gpu_ids: [2]

# Datasets
dataset:
  name: MIT_FiveK_4K
  random_reverse: true #false
  dataset_root: /data/990pro1/MIT_FiveK_4K
  n_workers: 8  # per GPU
  batch_size: 1
  crop_size: None
  image_shape: [3, 2680, 3840]
  use_flip: true
  use_rot: true

# Transformer structures
transformer:
  input_channel: 3
  output_channel: 9
  num_classes: 3
  embed_dim: 32
  depths: [1, 1, 1, 1, 1, 1, 1, 1]
  num_heads: [2, 4, 8, 16, 16, 8, 4, 2]
  window_sizes: [[4,4], [4,4], [4,4], [4,4], [4,4], [4,4], [4,4], [4,4]]
  back_RBs: 0
  recon_type: enhancement

# Filter structures
filter:
  low_freq_resolution: 64
  num_residual_blocks: 1
  num_lap: 6
  channels: 3

# LUT structures
LUT:
  LUT_dim: 33
  lambda_smooth: 0.0001
  lambda_mono: 10.0

# Path
path:
  pretrain_model: ~
  strict_load: true # false
  resume_state: ~

# Training settings
train:
  lr: 0.0002
  lr_scheme: CosineAnnealingLR_Restart
  beta1: 0.9
  beta2: 0.99
  niter: 1350000
  warmup_iter: -1 # 5000  # -1: no warm up
  T_period: [180000, 360000, 540000, 720000]
  restarts: [180000, 360000, 540000]
  restart_weights: [0.5, 0.5, 0.5]
  eta_min: !!float 1e-7
  manual_seed: 3407
  lambda_recon: 1.0
  lambda_lpips: 0

# Logger
logger:
  print_freq: 225000
  save_checkpoint_freq: 45000
